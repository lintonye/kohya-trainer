{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.lora-dreambooth) [![GitHub](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/Linaqruf/kohya-trainer) \n",
        "\n",
        "# **Kohya LoRA Dreambooth**\n",
        "A Colab Notebook For LoRA Training (Dreambooth Method)\n",
        "\n",
        "<details>\n",
        "  <summary><big>Support Us!</big></summary>\n",
        "    <ul>\n",
        "      <li>\n",
        "        <a href=\"https://ko-fi.com/linaqruf\">\n",
        "          <img src=\"https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat\" alt=\"Ko-fi badge\">\n",
        "        </a>\n",
        "      </li>\n",
        "      <li>\n",
        "        <a href=\"https://saweria.co/linaqruf\">\n",
        "          <img src=\"https://img.shields.io/badge/Saweria-7B3F00?style=flat&logo=ko-fi&logoColor=white\" alt=\"Saweria badge\">\n",
        "        </a>\n",
        "      </li>\n",
        "    </ul>\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_u3q60di584x"
      },
      "outputs": [],
      "source": [
        "#@title ## 1.1. Install Dependencies\n",
        "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.  This will also install the required libraries.\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "#root_dir\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir,\"deps\")\n",
        "repo_dir = os.path.join(root_dir,\"kohya-trainer\")\n",
        "training_dir = os.path.join(root_dir,\"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir,\"pretrained_model\")\n",
        "vae_dir = os.path.join(root_dir,\"vae\")\n",
        "config_dir = os.path.join(training_dir,\"config\")\n",
        "gdrive_root_dir = '/content/drive/My Drive/SDTraining/LoRa'\n",
        "\n",
        "#repo_dir\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir,\"tools\")\n",
        "finetune_dir = os.path.join(repo_dir,\"finetune\")\n",
        "\n",
        "for store in [\"root_dir\", \"deps_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"vae_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "  with capture.capture_output() as cap:\n",
        "    %store {store}\n",
        "    del cap\n",
        "\n",
        "repo_url = \"https://github.com/lintonye/kohya-trainer\"\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "mount_drive = True ##@param {type: \"boolean\"}\n",
        "\n",
        "if mount_drive:\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "for dir in [deps_dir, training_dir, config_dir, pretrained_model, vae_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "  \n",
        "def clone_repo(url):\n",
        "  if not os.path.exists(repo_dir):\n",
        "    os.chdir(root_dir)\n",
        "    !git clone {url} {repo_dir}\n",
        "  else:\n",
        "    os.chdir(repo_dir)\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "\n",
        "clone_repo(repo_url)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  with capture.capture_output() as cap:\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, 'r') as deps:\n",
        "      deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "    del cap \n",
        "\n",
        "def install_dependencies():\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "  if install_xformers:\n",
        "    !pip install -q --pre xformers\n",
        "    !pip install -q --pre triton\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "install_dependencies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHNbl3O_NSS0"
      },
      "source": [
        "# V. Training Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y"
      },
      "outputs": [],
      "source": [
        "# Check models\n",
        "pretrained_dir = f'{gdrive_root_dir}/pretrained' #@param {type:'string'}\n",
        "model_filename = 'Realistic_Vision_1.3.safetensors' #@param {type:'string'}\n",
        "pretrained_model_path = f\"{pretrained_dir}/{model_filename}\"\n",
        "#TODO check if model_path exists\n",
        "\n",
        "#@title ## 3.1. Locating Train Data Directory\n",
        "#@markdown Define the location of your training data. Regularization Images is `optional` and can be skipped.\n",
        "import os\n",
        "# from IPython.utils import capture\n",
        "%store -r\n",
        "\n",
        "train_data_dir = f\"{gdrive_root_dir}/datasets/yayawu77/5_yayawu77\" #@param {type:'string'}\n",
        "reg_data_dir = f\"{gdrive_root_dir}/datasets/reg_data\" #@param {type:'string'}\n",
        "\n",
        "# for image_dir in [train_data_dir, reg_data_dir]:\n",
        "#   if image_dir:\n",
        "#     with capture.capture_output() as cap:\n",
        "#       os.makedirs(image_dir, exist_ok=True)\n",
        "#       %store image_dir\n",
        "#       del cap\n",
        "\n",
        "print(f\"Your train data directory : {train_data_dir}\")\n",
        "if reg_data_dir:\n",
        "  print(f\"Your reg data directory : {reg_data_dir}\")\n",
        "\n",
        "\n",
        "##############################################################\n",
        "\n",
        "\n",
        "#@title ## 5.1. Model Config\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "project_name = \"yayawu77\" #@param {type:\"string\"}\n",
        "if not project_name:\n",
        "  project_name = \"last\"\n",
        "pretrained_model_name_or_path = pretrained_model_path\n",
        "vae = \"\"  #@param {type:\"string\"}\n",
        "output_dir = f\"{gdrive_root_dir}/output\" #@param {'type':'string'}\n",
        "project_output_dir = f'{output_dir}/{project_name}'\n",
        "\n",
        "sample_dir = os.path.join(project_output_dir, \"sample\")\n",
        "for dir in [project_output_dir, sample_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(\"Project Name: \", project_name)\n",
        "print(\"Model Version: Stable Diffusion V1.x\") if not v2 else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x\") if v2 and not v_parameterization else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x 768v\") if v2 and v_parameterization else \"\"\n",
        "print(\"Pretrained Model Path: \", pretrained_model_name_or_path) if pretrained_model_name_or_path else print(\"No Pretrained Model path specified.\")\n",
        "print(\"VAE Path: \", vae) if vae else print(\"No VAE path specified.\")\n",
        "print(\"Output Path: \", project_output_dir)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "#@title ## 5.2. Dataset Config\n",
        "import toml\n",
        "\n",
        "#@markdown ### Dreambooth Config\n",
        "train_repeats = 10 #@param {type:\"number\"}\n",
        "reg_repeats = 1 #@param {type:\"number\"}\n",
        "instance_token = \"yayawu77\" #@param {type:\"string\"}\t\n",
        "class_token = \"woman\" #@param {type:\"string\"}\t \n",
        "#@markdown ### <br>General Config\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "flip_aug = False #@param {type:\"boolean\"}\t\n",
        "caption_extension = \".txt\" #@param [\"none\", \".txt\", \".caption\"]\t\n",
        "caption_dropout_rate = 0 #@param {type:\"slider\", min:0, max:1, step:0.05}\t\n",
        "caption_dropout_every_n_epochs = 0 #@param {type:\"number\"}\n",
        "keep_tokens = 0 #@param {type:\"number\"}\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,           \n",
        "            \"caption_dropout_rate\": caption_dropout_rate if caption_extension == \".caption\" else 0,\n",
        "            \"caption_tag_dropout_rate\": caption_dropout_rate if caption_extension == \".txt\" else 0,\n",
        "            \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": [\n",
        "                {\n",
        "                    \"image_dir\": train_data_dir,\n",
        "                    \"class_tokens\": f\"{instance_token} {class_token}\",\n",
        "                    \"num_repeats\": train_repeats,\n",
        "                },\n",
        "                {\n",
        "                    \"is_reg\": True,\n",
        "                    \"image_dir\": reg_data_dir,\n",
        "                    \"class_tokens\": class_token,\n",
        "                    \"num_repeats\": reg_repeats,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "#@title ## 5.3. Sample Prompt Config\n",
        "enable_sample = True #@param {type:\"boolean\"}\n",
        "sample_every_n_type = \"sample_every_n_epochs\" #@param [\"sample_every_n_steps\", \"sample_every_n_epochs\"]\n",
        "sample_every_n_type_value = 1 #@param {type:\"number\"}\n",
        "if not enable_sample:\n",
        "  sample_every_n_type_value = 999999\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "prompt = f\"a close up portrait photo of {instance_token}, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\" #@param {type: \"string\"}\n",
        "negative = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\" #@param {type: \"string\"}\n",
        "width = \"512\" #@param {type:\"string\"}\n",
        "height = \"512\" #@param {type:\"string\"}\n",
        "scale = 7 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "steps = 28 #@param {type:\"number\"}\n",
        "\n",
        "sample_str = f\"\"\"\n",
        "  {prompt} \\\n",
        "  --n {negative} \\\n",
        "  --w {width} \\\n",
        "  --h {height} \\\n",
        "  --l {scale} \\\n",
        "  --s {steps} \\\n",
        "  {f\"--d \" + seed if seed > 0 else \"\"} \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "with open(prompt_path, \"w\") as f:\n",
        "    f.write(sample_str)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "#@title ## 5.4. LoRA and Optimizer Config\n",
        "\n",
        "#@markdown ### LoRA Config:\n",
        "#@markdown - `networks.lora` is normal and default [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts) LoRA.\n",
        "#@markdown - `lycoris.kohya` is a python package for LoRA module. Previously LoCon. Currently there are 2 LoRA algorithms: LoCon and LoRA with [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) representation. Put `algo=lora` for LoCon or `algo=loha` for Hadamard Product in `network_args`. Read: [KohakuBlueleaf/LyCORIS](https://github.com/KohakuBlueleaf/Lycoris).\n",
        "#@markdown - `locon.locon_kohya` <font color = 'red'> (backward compatibility, deprecated)</font> is LoRA for convolutional network. In short, it's the same LoRA but training almost all layers including normal LoRA layer. Read: [KohakuBlueleaf/LoCon](https://github.com/KohakuBlueleaf/LoCon).\n",
        "network_module = \"lycoris.kohya\" #@param [\"networks.lora\", \"lycoris.kohya\", \"locon.locon_kohya\"]\n",
        "\n",
        "#@markdown For custom `networks_module` you need to set additional `network_args`, e.g.: `[\"conv_dim=32\",\"conv_alpha=16\"]`\n",
        "network_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Some LoRA guides using 128 dim/alpha, but it's recommended to not specify `network_dim` and `alpha` higher than `48-64`. \n",
        "#@markdown The smaller `network_dim` is, the smaller the model size is. The larger `network_alpha` is, the closer the model is to a fully fine-tuned model. Read: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "network_dim = 32 #@param {'type':'number'}\n",
        "network_alpha = 16 #@param {'type':'number'}\n",
        "#@markdown You can specify this field for resume training.\n",
        "network_weight = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown ### <br>Optimizer Config:\n",
        "#@markdown `AdamW8bit` was the old `--use_8bit_adam`.\n",
        "optimizer_type = \"AdamW8bit\" #@param [\"AdamW\", \"AdamW8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation\", \"AdaFactor\"]\n",
        "#@markdown Additional arguments for optimizer, e.g: `[\"decouple=true\",\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Set `unet_lr` to `1.0` if you use `DAdaptation` optimizer, because it's a [free learning rate](https://github.com/facebookresearch/dadaptation) algorithm. \n",
        "#@markdown However `text_encoder_lr = 1/2 * unet_lr` still applied, so you need to set `0.5` for `text_encoder_lr`.\n",
        "#@markdown Also actually you don't need to specify `learning_rate` value if both `unet_lr` and `text_encoder_lr` are defined.\n",
        "train_unet = True #@param {'type':'boolean'}\n",
        "unet_lr = 1e-4 #@param {'type':'number'}\n",
        "train_text_encoder = True #@param {'type':'boolean'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"] {allow-input: false}\n",
        "lr_warmup_steps = 0 #@param {'type':'number'}\n",
        "#@markdown You can define `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial` in the field below.\n",
        "lr_scheduler_num_cycles = 0 #@param {'type':'number'}\n",
        "lr_scheduler_power = 0 #@param {'type':'number'}\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(\"network args:\", network_args)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if not network_weight:\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weight):\n",
        "    print(\"Loading LoRA weight:\", network_weight)\n",
        "  else:\n",
        "    print(f\"{network_weight} does not exist.\")\n",
        "    network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "  print(f\"Optimizer Args :\", optimizer_args)\n",
        "if train_unet and train_text_encoder:\n",
        "  print(f\"Train UNet and Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "if train_unet and not train_text_encoder:\n",
        "  print(f\"Train UNet only\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "if train_text_encoder and not train_unet:\n",
        "  print(f\"Train Text Encoder only\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "print(\"Learning rate warmup steps: \", lr_warmup_steps)\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- lr_scheduler_num_cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- lr_scheduler_power: \", lr_scheduler_power)\n",
        "\n",
        "\n",
        "##############################################################\n",
        "\n",
        "#@title ## 5.5. Training Config\n",
        "\n",
        "import toml\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "lowram = True #@param {type:\"boolean\"}\n",
        "noise_offset = 0.0 #@param {type:\"number\"}\n",
        "num_epochs = 10 #@param {type:\"number\"}\n",
        "train_batch_size = 6 #@param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_every_n_epochs\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/LoRA/logs\"\n",
        "prior_loss_weight = 1.0\n",
        "              \n",
        "os.chdir(repo_dir)\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": v2,\n",
        "        \"v_parameterization\": v_parameterization if v2 and v_parameterization else False,\n",
        "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": eval(network_args) if network_args else None,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": project_output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": True,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip if not v2 else None,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\":{\n",
        "        \"sample_every_n_steps\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_steps\" else None,\n",
        "        \"sample_every_n_epochs\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_epochs\" else None,\n",
        "        \"sample_sampler\": sampler,\n",
        "    },\n",
        "    \"dreambooth_arguments\":{\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\":{\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)\n",
        "\n",
        "\n",
        "##############################################################\n",
        "\n",
        "#@title ## 5.6. Start Training\n",
        "\n",
        "#@markdown You can import config from another session if you want.\n",
        "sample_prompt = \"/content/LoRA/config/sample_prompt.txt\" #@param {type:'string'}\n",
        "config_file = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
        "dataset_config = \"/content/LoRA/config/dataset_config.toml\" #@param {type:'string'}\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!accelerate launch \\\n",
        "  --config_file={accelerate_config} \\\n",
        "  --num_cpu_threads_per_process=1 \\\n",
        "  train_network.py \\\n",
        "  --sample_prompts={sample_prompt} \\\n",
        "  --dataset_config={dataset_config} \\\n",
        "  --config_file={config_file}\n",
        "\n",
        "\n",
        "##############################################################\n",
        "\n",
        "#@title ## 6.2. Inference\n",
        "%store -r\n",
        "\n",
        "#@markdown ### LoRA Config\n",
        "network_weight = \"\" #@param {'type':'string'}\n",
        "network_module = \"networks.lora\" #@param [\"networks.lora\", \"locon.locon_kohya\"]\n",
        "network_mul = 0.7 #@param {type:\"slider\", min:-1, max:2, step:0.05}\n",
        "network_args = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown ### <br> General Config\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "instance_prompt = \"\" #@param {type: \"string\"}\n",
        "model = pretrained_model_path #@param {type: \"string\"}\n",
        "# vae = \"\" #@param {type: \"string\"}\n",
        "outdir = f'{project_output_dir}/images' #@param {type: \"string\"}\n",
        "scale = 7 #@param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 28 #@param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512 #@param {type: \"integer\"}\n",
        "height = 768 #@param {type: \"integer\"}\n",
        "images_per_prompt = 4 #@param {type: \"integer\"}\n",
        "batch_size = 4 #@param {type: \"integer\"}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "\n",
        "final_prompt = f\"{instance_prompt}, {prompt} --n {negative}\" if instance_prompt else f\"{prompt} --n {negative}\"\n",
        "\n",
        "config = {\n",
        "  \"v2\": v2,\n",
        "  \"v_parameterization\": v_parameterization,\n",
        "  \"network_module\": network_module,\n",
        "  \"network_weight\": network_weight,\n",
        "  \"network_mul\": float(network_mul),\n",
        "  \"network_args\": eval(network_args) if network_args else None,\n",
        "  \"ckpt\": model,\n",
        "  \"outdir\": outdir,\n",
        "  \"xformers\": True,\n",
        "  \"vae\": vae if vae else None,\n",
        "  \"fp16\": True,\n",
        "  \"W\": width,\n",
        "  \"H\": height,\n",
        "  \"seed\": seed if seed > 0 else None,\n",
        "  \"scale\": scale,\n",
        "  \"sampler\": sampler,\n",
        "  \"steps\": steps,\n",
        "  \"max_embeddings_multiples\": 3,\n",
        "  \"batch_size\": batch_size,\n",
        "  \"images_per_prompt\": images_per_prompt,\n",
        "  \"clip_skip\": clip_skip if not v2 else None,\n",
        "  \"prompt\": final_prompt,\n",
        "}\n",
        "\n",
        "args = \"\"\n",
        "for k,v in config.items():\n",
        "  if isinstance(v, str):\n",
        "    args += f\"--{k}=\\\"{v}\\\" \"\n",
        "  if isinstance(v, bool) and v:\n",
        "    args += f\"--{k} \"\n",
        "  if isinstance(v, float) and not isinstance(v, bool):\n",
        "    args += f\"--{k}={v} \"\n",
        "  if isinstance(v, int) and not isinstance(v, bool):\n",
        "    args += f\"--{k}={v} \"\n",
        "\n",
        "final_args = f\"python gen_img_diffusers.py {args}\"\n",
        "\n",
        "!{final_args}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMcN0bM_o53"
      },
      "source": [
        "# VI. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rt7CKCog_4tm"
      },
      "outputs": [],
      "source": [
        "#@title ## 6.1. Interrogating LoRA Weights\n",
        "#@markdown Now you can check if your LoRA trained properly. \n",
        "\n",
        "#@markdown  If you used `clip_skip = 2` during training, the values of `lora_te_text_model_encoder_layers_11_*` will be `0.0`, this is normal. These layers are not trained at this value of `Clip Skip`.\n",
        "network_weight = \"\" #@param {'type':'string'}\n",
        "no_verbose = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from safetensors.torch import load_file\n",
        "from safetensors.torch import safe_open\n",
        "import library.model_util as model_util\n",
        "\n",
        "print(\"Loading LoRA weight:\", network_weight)\n",
        "\n",
        "def main(file, verbose:bool):\n",
        "  if not verbose:\n",
        "    sd = load_file(file) if os.path.splitext(file)[1] == '.safetensors' else torch.load(file, map_location='cuda')\n",
        "    values = []\n",
        "\n",
        "    keys = list(sd.keys())\n",
        "    for key in keys:\n",
        "      if 'lora_up' in key or 'lora_down' in key:\n",
        "        values.append((key, sd[key]))\n",
        "    print(f\"number of LoRA modules: {len(values)}\")\n",
        "\n",
        "    for key, value in values:\n",
        "      value = value.to(torch.float32)\n",
        "      print(f\"{key},{torch.mean(torch.abs(value))},{torch.min(torch.abs(value))}\")\n",
        "  \n",
        "  if model_util.is_safetensors(file):\n",
        "      with safe_open(file, framework=\"pt\") as f:\n",
        "          metadata = f.metadata()\n",
        "      if metadata is not None:\n",
        "        print(f\"\\nLoad metadata for: {file}\")\n",
        "        print(json.dumps(metadata, indent=4))\n",
        "  else:\n",
        "      print(\"No metadata saved, your model is not in safetensors format\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main(network_weight, no_verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TLSQslfFcQde"
      },
      "outputs": [],
      "source": [
        "#@title ## 6.4. Visualize loss graph (Optional)\n",
        "training_logs_path = \"/content/LoRA/logs\" #@param {type : \"string\"}\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {training_logs_path}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
